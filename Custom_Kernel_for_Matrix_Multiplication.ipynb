{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XkFQVpF_zrhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866f2e90-16c0-4106-e167-a634ca204c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU RAM Free: 11.7 GB\n",
            "GPU 0 ... Mem Free: 14861MB / 15360MB | Utilization   2%\n"
          ]
        }
      ],
      "source": [
        "!pip -q install gputil psutil humanize\n",
        "# Import packages\n",
        "import os,sys,humanize,psutil,GPUtil\n",
        "\n",
        "# Define function\n",
        "def mem_report():\n",
        "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "\n",
        "  GPUs = GPUtil.getGPUs()\n",
        "  for i, gpu in enumerate(GPUs):\n",
        "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'\n",
        "    .format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "\n",
        "# Execute function\n",
        "mem_report()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbm9GkuB8Y_W",
        "outputId": "6d3d6ff1-ca3b-4629-a0ec-2090ae6b5a6d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 30 00:32:13 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0              28W /  70W |    241MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tesla T4 → Compute Capability 7.5 → Architecture 7.5"
      ],
      "metadata": {
        "id": "VSb3QRb_8fVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TORCH_CUDA_ARCH_LIST'] = \"7.5\""
      ],
      "metadata": {
        "id": "mNs6TnfL8l1d"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3-pybind11"
      ],
      "metadata": {
        "id": "0dWFoh1d4Imi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matmul_kernel.cu\n",
        "\n",
        "#include <torch/extension.h>\n",
        "\n",
        "template <typename T>\n",
        "__global__ void matmul_kernel(const T* A, const T* B, T* C, int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (row < N && col < N) {\n",
        "        T sum = 0;\n",
        "        for (int k = 0; k < N; ++k) {\n",
        "            sum += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "void matmul_launcher(torch::Tensor A, torch::Tensor B, torch::Tensor C, int N) {\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 numBlocks((N + 15) / 16, (N + 15) / 16);\n",
        "    matmul_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<T>(), B.data_ptr<T>(), C.data_ptr<T>(), N);\n",
        "}\n",
        "\n",
        "void matmul_binding(torch::Tensor A, torch::Tensor B, torch::Tensor C, int N) {\n",
        "    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), \"matmul_launcher\", ([&] {\n",
        "        matmul_launcher<scalar_t>(A, B, C, N);\n",
        "    }));\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"matmul\", &matmul_binding, \"Matrix multiplication kernel\");\n",
        "}\n"
      ],
      "metadata": {
        "id": "5CsUlMk41lR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "matmul_kernel = load(\n",
        "    name=\"matmul_kernel\",\n",
        "    sources=[\"matmul_kernel.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\"]\n",
        ")"
      ],
      "metadata": {
        "id": "IN9dgIfU7N1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def matmul_cuda(A, B, N):\n",
        "    C = torch.zeros((N, N), dtype=A.dtype, device='cuda')\n",
        "    matmul_kernel.matmul(A, B, C, N)\n",
        "    return C\n"
      ],
      "metadata": {
        "id": "7idQlu6i7yK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 2048\n",
        "A = torch.randn((N, N), dtype=torch.float32, device='cuda')\n",
        "B = torch.randn((N, N), dtype=torch.float32, device='cuda')\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "C_pytorch = torch.matmul(A, B)\n",
        "print(\"PyTorch Time:\", time.time() - start)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "C_cuda = matmul_cuda(A, B, N)\n",
        "print(\"CUDA Kernel Time:\", time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wgHZIgb85UD",
        "outputId": "d4bc31a9-0248-4837-b383-c0b938699ed1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Time: 0.0005533695220947266\n",
            "CUDA Kernel Time: 0.0004603862762451172\n"
          ]
        }
      ]
    }
  ]
}